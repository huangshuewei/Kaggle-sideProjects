{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-11T13:37:28.909406Z","iopub.execute_input":"2023-08-11T13:37:28.909793Z","iopub.status.idle":"2023-08-11T13:37:28.919465Z","shell.execute_reply.started":"2023-08-11T13:37:28.909760Z","shell.execute_reply":"2023-08-11T13:37:28.918280Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/input/textiledefectdetection/train64.h5\n/kaggle/input/textiledefectdetection/matchingtDATASET_test_32.h5\n/kaggle/input/textiledefectdetection/test32.csv\n/kaggle/input/textiledefectdetection/train32.csv\n/kaggle/input/textiledefectdetection/test32.h5\n/kaggle/input/textiledefectdetection/train32.h5\n/kaggle/input/textiledefectdetection/test64.csv\n/kaggle/input/textiledefectdetection/matchingtDATASET_train_32.h5\n/kaggle/input/textiledefectdetection/matchingtDATASET_test_64.h5\n/kaggle/input/textiledefectdetection/matchingtDATASET_train_64.h5\n/kaggle/input/textiledefectdetection/test64.h5\n/kaggle/input/textiledefectdetection/train64.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/textiledefectdetection/train32.csv')\ntest = pd.read_csv('/kaggle/input/textiledefectdetection/test32.csv')\nprint(train)\nprint(test)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:37:28.948569Z","iopub.execute_input":"2023-08-11T13:37:28.948841Z","iopub.status.idle":"2023-08-11T13:37:29.031367Z","shell.execute_reply.started":"2023-08-11T13:37:28.948817Z","shell.execute_reply":"2023-08-11T13:37:29.030271Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"       index  angle indication_type  indication_value  split\n0      48000     40            good                 0  train\n1      48001     60            good                 0  train\n2      48002     20            good                 0  train\n3      48003     40            good                 0  train\n4      48004     20            good                 0  train\n...      ...    ...             ...               ...    ...\n47995  95995     20          thread                 4  train\n47996  95996    100          thread                 4  train\n47997  95997    120          thread                 4  train\n47998  95998    120          thread                 4  train\n47999  95999     20          thread                 4  train\n\n[48000 rows x 5 columns]\n       index  angle indication_type  indication_value split\n0          0     40            good                 0  test\n1          1     80            good                 0  test\n2          2     60            good                 0  test\n3          3      0            good                 0  test\n4          4    140            good                 0  test\n...      ...    ...             ...               ...   ...\n47995  47995     20          thread                 4  test\n47996  47996    100          thread                 4  test\n47997  47997    100          thread                 4  test\n47998  47998    140          thread                 4  test\n47999  47999    120          thread                 4  test\n\n[48000 rows x 5 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 0: good textile, 1: damaged textile\n# Train Data\ntrain['index'] = train['index'] - 48000\ntrain_good = train[train['indication_value'] == 0]\ntrain_damaged = train[train['indication_value'] != 0]\ntrain_damaged['indication_value'] = 1\n\n# Test Data\ntest['index'] = test['index']\ntest_good = test[test['indication_value'] == 0]\ntest_damaged = test[test['indication_value'] != 0]\ntest_damaged['indication_value'] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:37:29.035038Z","iopub.execute_input":"2023-08-11T13:37:29.035758Z","iopub.status.idle":"2023-08-11T13:37:29.055315Z","shell.execute_reply.started":"2023-08-11T13:37:29.035719Z","shell.execute_reply":"2023-08-11T13:37:29.054140Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/904540034.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train_damaged['indication_value'] = 1\n/tmp/ipykernel_28/904540034.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test_damaged['indication_value'] = 1\n","output_type":"stream"}]},{"cell_type":"code","source":"print(test_good.count())\nprint(test_damaged[test['angle'] == 120].count())\nprint(test_damaged[test['angle'] == 20].count())","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:37:29.057190Z","iopub.execute_input":"2023-08-11T13:37:29.057596Z","iopub.status.idle":"2023-08-11T13:37:29.087498Z","shell.execute_reply.started":"2023-08-11T13:37:29.057560Z","shell.execute_reply":"2023-08-11T13:37:29.086504Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"index               8000\nangle               8000\nindication_type     8000\nindication_value    8000\nsplit               8000\ndtype: int64\nindex               5028\nangle               5028\nindication_type     5028\nindication_value    5028\nsplit               5028\ndtype: int64\nindex               4991\nangle               4991\nindication_type     4991\nindication_value    4991\nsplit               4991\ndtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/486680720.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  print(test_damaged[test['angle'] == 120].count())\n/tmp/ipykernel_28/486680720.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  print(test_damaged[test['angle'] == 20].count())\n","output_type":"stream"}]},{"cell_type":"code","source":"train_table = pd.concat([train_good, train_damaged[train['angle'] == 20], train_damaged[train['angle'] == 120]])\ntest_table = pd.concat([test_good, test_damaged[test['angle'] == 20], test_damaged[test['angle'] == 120]])","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:37:29.089764Z","iopub.execute_input":"2023-08-11T13:37:29.090648Z","iopub.status.idle":"2023-08-11T13:37:29.116976Z","shell.execute_reply.started":"2023-08-11T13:37:29.090613Z","shell.execute_reply":"2023-08-11T13:37:29.115908Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/291709261.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  train_table = pd.concat([train_good, train_damaged[train['angle'] == 20], train_damaged[train['angle'] == 120]])\n/tmp/ipykernel_28/291709261.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  test_table = pd.concat([test_good, test_damaged[test['angle'] == 20], test_damaged[test['angle'] == 120]])\n","output_type":"stream"}]},{"cell_type":"code","source":"import h5py\nimport keras\n\nf = h5py.File('/kaggle/input/textiledefectdetection/train32.h5', 'r')\na_group_key = list(f.keys())[0]\ndata = list(f[a_group_key])\n\nx_train = []\ny_train = []\nidx = train_table['index'].astype('int')\nindication_value = train_table['indication_value'].astype('int')\nfor i in idx:\n    x_train.append(data[i])\n    y_train.append(indication_value.loc[idx[i]])\n    \nx_train = np.array(x_train)\ny_train = np.array(y_train)\ny_train = keras.utils.to_categorical(y_train, num_classes=2)\n\nprint(x_train.shape)\nprint(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:37:29.118547Z","iopub.execute_input":"2023-08-11T13:37:29.119245Z","iopub.status.idle":"2023-08-11T13:37:32.690940Z","shell.execute_reply.started":"2023-08-11T13:37:29.119205Z","shell.execute_reply":"2023-08-11T13:37:32.689924Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(18208, 32, 32, 1)\n(18208, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"f = h5py.File('/kaggle/input/textiledefectdetection/test32.h5', 'r')\na_group_key = list(f.keys())[0]\ndata = list(f[a_group_key])\n\nx_test = []\ny_test = []\nidx = test_table['index'].astype('int')\nindication_value = test_table['indication_value'].astype('int')\nfor i in idx:\n    x_test.append(data[i])\n    y_test.append(indication_value.loc[idx[i]])\n    \nx_test = np.array(x_test)\ny_test = np.array(y_test)\ny_test = keras.utils.to_categorical(y_test, num_classes=2)\n\nprint(x_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:37:32.692722Z","iopub.execute_input":"2023-08-11T13:37:32.693119Z","iopub.status.idle":"2023-08-11T13:37:36.300996Z","shell.execute_reply.started":"2023-08-11T13:37:32.693083Z","shell.execute_reply":"2023-08-11T13:37:36.299971Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(18019, 32, 32, 1)\n(18019, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Build model\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torchvision.models import resnet18\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Build ResNet50 model\nmodel = resnet18(pretrained = True)\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),bias=False)\n\n# Connect to fully connnected layer\nmodel.fc = nn.Sequential(\n    nn.Linear(512,512),\n    nn.ReLU(inplace=True),\n    nn.Linear(512,64),\n    nn.ReLU(inplace=True),\n    nn.Linear(64,2)\n)\n\ndef init_weights(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\nmodel.apply(init_weights)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:37:36.303645Z","iopub.execute_input":"2023-08-11T13:37:36.305181Z","iopub.status.idle":"2023-08-11T13:37:36.633819Z","shell.execute_reply.started":"2023-08-11T13:37:36.305142Z","shell.execute_reply":"2023-08-11T13:37:36.632667Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Sequential(\n    (0): Linear(in_features=512, out_features=512, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Linear(in_features=512, out_features=64, bias=True)\n    (3): ReLU(inplace=True)\n    (4): Linear(in_features=64, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Set data loader\nclass cifar10Dataset(Dataset):\n    def __init__(self, imgs, labels, transform=None):\n        self.imgs = imgs\n        self.labels = labels\n        self.transforms = transform\n\n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, index):\n        x = self.imgs[index]\n        y = self.labels[index]\n\n        if self.transforms:\n            x = self.transforms(x)\n\n        x = x.float()\n        return x, y\n\n# Split train and validation\nx_train, x_val, y_train, y_val = train_test_split(x_train, \n                                                  y_train, \n                                                  test_size=0.2, \n                                                  random_state=100,\n                                                  shuffle=True)\n\nlearningRate = 0.001\nbatch_size = 64\n\n# Set loss function and optimiser\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),lr = learningRate)\n\n# stats = ((0.5), (0.5))\ntransform = transforms.Compose([transforms.ToTensor(),\n                                # transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n                                # transforms.RandomHorizontalFlip(),\n                                # transforms.Normalize(*stats,inplace=True)\n                               ])\n\ntrain_dataset = cifar10Dataset(x_train, y_train, transform)\nval_dataset = cifar10Dataset(x_val, y_val, transform)\ntest_dataset = cifar10Dataset(x_test, y_test, transform)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size= batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size= batch_size, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:37:36.635582Z","iopub.execute_input":"2023-08-11T13:37:36.635990Z","iopub.status.idle":"2023-08-11T13:37:36.667976Z","shell.execute_reply.started":"2023-08-11T13:37:36.635954Z","shell.execute_reply":"2023-08-11T13:37:36.667017Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Train loop\nnum_epoch = 25\nbest_val_accuracy = 0.0\n\nfor epoch in range(num_epoch):\n    model.train()\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            _, labels = torch.max(labels, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        val_accuracy = correct / total\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            torch.save(model.state_dict(), 'best_model.pth')\n        \n        print(f'Epoch [{epoch+1}/{num_epoch}], Validation Accuracy: {val_accuracy:.4f}, BEST Accuracy: {best_val_accuracy:.4f}')\n\nprint('Training Finished!')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:37:36.669472Z","iopub.execute_input":"2023-08-11T13:37:36.669831Z","iopub.status.idle":"2023-08-11T13:39:23.790240Z","shell.execute_reply.started":"2023-08-11T13:37:36.669798Z","shell.execute_reply":"2023-08-11T13:39:23.789078Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch [1/25], Validation Accuracy: 0.6790, BEST Accuracy: 0.6790\nEpoch [2/25], Validation Accuracy: 0.7485, BEST Accuracy: 0.7485\nEpoch [3/25], Validation Accuracy: 0.6433, BEST Accuracy: 0.7485\nEpoch [4/25], Validation Accuracy: 0.7672, BEST Accuracy: 0.7672\nEpoch [5/25], Validation Accuracy: 0.8284, BEST Accuracy: 0.8284\nEpoch [6/25], Validation Accuracy: 0.8366, BEST Accuracy: 0.8366\nEpoch [7/25], Validation Accuracy: 0.8092, BEST Accuracy: 0.8366\nEpoch [8/25], Validation Accuracy: 0.8471, BEST Accuracy: 0.8471\nEpoch [9/25], Validation Accuracy: 0.8548, BEST Accuracy: 0.8548\nEpoch [10/25], Validation Accuracy: 0.8229, BEST Accuracy: 0.8548\nEpoch [11/25], Validation Accuracy: 0.8616, BEST Accuracy: 0.8616\nEpoch [12/25], Validation Accuracy: 0.6260, BEST Accuracy: 0.8616\nEpoch [13/25], Validation Accuracy: 0.5988, BEST Accuracy: 0.8616\nEpoch [14/25], Validation Accuracy: 0.8064, BEST Accuracy: 0.8616\nEpoch [15/25], Validation Accuracy: 0.6431, BEST Accuracy: 0.8616\nEpoch [16/25], Validation Accuracy: 0.8243, BEST Accuracy: 0.8616\nEpoch [17/25], Validation Accuracy: 0.8421, BEST Accuracy: 0.8616\nEpoch [18/25], Validation Accuracy: 0.6834, BEST Accuracy: 0.8616\nEpoch [19/25], Validation Accuracy: 0.6549, BEST Accuracy: 0.8616\nEpoch [20/25], Validation Accuracy: 0.7938, BEST Accuracy: 0.8616\nEpoch [21/25], Validation Accuracy: 0.8756, BEST Accuracy: 0.8756\nEpoch [22/25], Validation Accuracy: 0.8564, BEST Accuracy: 0.8756\nEpoch [23/25], Validation Accuracy: 0.8987, BEST Accuracy: 0.8987\nEpoch [24/25], Validation Accuracy: 0.8627, BEST Accuracy: 0.8987\nEpoch [25/25], Validation Accuracy: 0.8405, BEST Accuracy: 0.8987\nTraining Finished!\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\ntotal = 0\ncorrect = 0\n\nfor images, labels in test_loader:\n    images, labels = images.to(device), labels.to(device)\n    batch_outputs = model(images) \n    _, predicted = torch.max(batch_outputs, 1)\n    _, labels = torch.max(labels, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()\n    \ntest_accuracy = correct / total\nprint('Test accuracy: ', test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:39:23.792085Z","iopub.execute_input":"2023-08-11T13:39:23.792508Z","iopub.status.idle":"2023-08-11T13:39:25.455713Z","shell.execute_reply.started":"2023-08-11T13:39:23.792470Z","shell.execute_reply":"2023-08-11T13:39:25.454626Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Test accuracy:  0.8773516843332039\n","output_type":"stream"}]}]}